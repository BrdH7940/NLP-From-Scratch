{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementing the Neural Network for Transition-Based Dependency Parsing\n",
        "\n",
        "This notebook implements the feedforward neural network model described in the CS224n Lecture Notes, Part IV (Dependency Parsing 2, Winter 2019), specifically Section 1.4 and Figure 3.\n",
        "\n",
        "**Goal:** To build a neural network that, given features extracted from a parser's state (stack, buffer, existing arcs), predicts the next optimal transition (e.g., SHIFT, LEFT-ARC, RIGHT-ARC) to take in order to construct a dependency tree.\n",
        "\n",
        "**Focus:** This implementation focuses *only* on the neural network architecture itself. It does not implement the actual transition system logic (managing the stack, buffer, and arcs). We will simulate the input features that such a system would provide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "We'll use PyTorch to build the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understanding the Input Features\n",
        "\n",
        "The lecture notes describe extracting features from the current parser configuration `c = (σ, β, A)` (stack, buffer, arcs).\n",
        "\n",
        "**Feature Types:**\n",
        "1.  `Sword`: Specific words from the top of the stack, front of the buffer, and possibly children/dependents of words on the stack.\n",
        "2.  `Stag`: Part-of-Speech (POS) tags corresponding to the words selected for `Sword`.\n",
        "3.  `Slabel`: Dependency relation labels for arcs connected to the selected words (e.g., labels of children).\n",
        "\n",
        "**Example Feature Set (from Page 4):**\n",
        "*   `Sword`: 18 specific word positions (e.g., top 3 stack, top 3 buffer, children of top 2 stack, grandchildren).\n",
        "*   `Stag`: 18 corresponding POS tags.\n",
        "*   `Slabel`: 12 corresponding arc labels (for the children/grandchildren positions).\n",
        "\n",
        "**Representation:**\n",
        "*   Each actual word, tag, or label is mapped to a unique integer ID.\n",
        "*   A special `NULL` token/ID is used for positions where no word/tag/label exists (e.g., empty stack/buffer, no children).\n",
        "*   The input to the network for a given state `c` is a set of these integer IDs, corresponding to the pre-defined feature template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Defining Model Parameters and Hyperparameters\n",
        "\n",
        "We need to define the sizes of our vocabularies, the embedding dimensions, hidden layer size, and the number of possible output transitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vocabulary sizes (including NULL token)\n",
        "WORD_VOCAB_SIZE = 10000  # Example size\n",
        "TAG_SET_SIZE = 50       # Example size (e.g., Penn Treebank tags + NULL)\n",
        "LABEL_SET_SIZE = 40     # Example size (e.g., Universal Dependencies labels + NULL)\n",
        "\n",
        "# Feature counts (based on the example in the notes)\n",
        "NUM_WORD_FEATURES = 18\n",
        "NUM_TAG_FEATURES = 18\n",
        "NUM_LABEL_FEATURES = 12\n",
        "\n",
        "# Dimensions\n",
        "EMBEDDING_DIM = 50      # Dimension 'd' for all embeddings\n",
        "HIDDEN_DIM = 200        # Size of the hidden layer\n",
        "\n",
        "# Output size: Number of possible transitions\n",
        "# Example: 1 SHIFT + (LABEL_SET_SIZE-1) LEFT-ARCs + (LABEL_SET_SIZE-1) RIGHT-ARCs \n",
        "# (Subtract 1 because NULL label isn't used for actual arcs)\n",
        "NUM_TRANSITIONS = 1 + (LABEL_SET_SIZE - 1) + (LABEL_SET_SIZE - 1) \n",
        "\n",
        "# Training Hyperparameters (example)\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. The Neural Network Model Architecture\n",
        "\n",
        "The model follows Figure 3:\n",
        "1.  **Input Layer:** Takes integer IDs for the selected word, tag, and label features.\n",
        "2.  **Embedding Layer:** Maps each integer ID to a dense vector using embedding matrices (`Ew`, `Et`, `El`).\n",
        "3.  **Concatenation:** Concatenates all the retrieved embedding vectors into a single large input vector `[xw, xt, xl]`.\n",
        "4.  **Hidden Layer:** Applies an affine transformation (`W1`, `b1`) followed by a non-linear activation function. The notes use `f(x) = x^3`.\n",
        "   *   $h = (W_1^w x^w + W_1^t x^t + W_1^l x^l + b_1)^3$\n",
        "   *   Alternatively, concatenate first: $h = (W_1 [x^w, x^t, x^l] + b_1)^3$\n",
        "   We will implement the second version (concatenate first) for simplicity, which is equivalent if `W1` is structured appropriately.\n",
        "5.  **Output Layer:** Applies another affine transformation (`W2`, `b2`) to produce scores (logits) for each possible transition.\n",
        "   *   $logits = W_2 h + b_2$\n",
        "6.  **Softmax Layer (Implicit in Loss):** A softmax function is applied to the logits to get probabilities for each transition. This is typically combined with the Cross-Entropy loss function during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralDependencyParserModel(nn.Module):\n",
        "    def __init__(self, word_vocab_size, tag_set_size, label_set_size, \n",
        "                 num_word_features, num_tag_features, num_label_features,\n",
        "                 embedding_dim, hidden_dim, num_transitions):\n",
        "        super(NeuralDependencyParserModel, self).__init__()\n",
        "\n",
        "        # Embedding layers\n",
        "        self.word_embedding = nn.Embedding(word_vocab_size, embedding_dim)\n",
        "        self.tag_embedding = nn.Embedding(tag_set_size, embedding_dim)\n",
        "        self.label_embedding = nn.Embedding(label_set_size, embedding_dim)\n",
        "\n",
        "        # Calculate the total size of the concatenated embedding vector\n",
        "        self.total_input_dim = (num_word_features + num_tag_features + num_label_features) * embedding_dim\n",
        "\n",
        "        # Linear layers\n",
        "        # Layer 1: Maps concatenated embeddings to hidden layer\n",
        "        self.layer1 = nn.Linear(self.total_input_dim, hidden_dim)\n",
        "        # Layer 2: Maps hidden layer to output logits (scores for each transition)\n",
        "        self.layer2 = nn.Linear(hidden_dim, num_transitions)\n",
        "\n",
        "        # Optional: Initialize weights (e.g., Xavier initialization)\n",
        "        nn.init.xavier_uniform_(self.layer1.weight)\n",
        "        nn.init.constant_(self.layer1.bias, 0.1)\n",
        "        nn.init.xavier_uniform_(self.layer2.weight)\n",
        "        nn.init.constant_(self.layer2.bias, 0.1)\n",
        "\n",
        "    def forward(self, word_features, tag_features, label_features):\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the network.\n",
        "        \n",
        "        Args:\n",
        "            word_features (torch.Tensor): Tensor of word indices (batch_size, num_word_features).\n",
        "            tag_features (torch.Tensor): Tensor of tag indices (batch_size, num_tag_features).\n",
        "            label_features (torch.Tensor): Tensor of label indices (batch_size, num_label_features).\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Logits for each possible transition (batch_size, num_transitions).\n",
        "        \"\"\"\n",
        "        # 1. Get embeddings for all features\n",
        "        # Shape: (batch_size, num_features, embedding_dim)\n",
        "        word_embeds = self.word_embedding(word_features)\n",
        "        tag_embeds = self.tag_embedding(tag_features)\n",
        "        label_embeds = self.label_embedding(label_features)\n",
        "\n",
        "        # 2. Flatten embeddings before concatenation\n",
        "        # Shape: (batch_size, num_features * embedding_dim)\n",
        "        batch_size = word_features.size(0)\n",
        "        word_flat = word_embeds.view(batch_size, -1)\n",
        "        tag_flat = tag_embeds.view(batch_size, -1)\n",
        "        label_flat = label_embeds.view(batch_size, -1)\n",
        "\n",
        "        # 3. Concatenate all feature embeddings\n",
        "        # Shape: (batch_size, total_input_dim)\n",
        "        combined_embeds = torch.cat((word_flat, tag_flat, label_flat), dim=1)\n",
        "\n",
        "        # 4. Hidden Layer: Affine transformation + Cubic Activation\n",
        "        # Shape: (batch_size, hidden_dim)\n",
        "        h = self.layer1(combined_embeds)\n",
        "        h_activated = torch.pow(h, 3) # Cubic activation as per Figure 3\n",
        "        # Alternatively, use other activations like ReLU: \n",
        "        # h_activated = torch.relu(h)\n",
        "\n",
        "        # 5. Output Layer: Affine transformation to get logits\n",
        "        # Shape: (batch_size, num_transitions)\n",
        "        logits = self.layer2(h_activated)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Simulating Input Data and Training Step\n",
        "\n",
        "Since we don't have a real parser state generator, we will create random dummy input data (feature indices) and target transitions to demonstrate how the model would be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "model = NeuralDependencyParserModel(\n",
        "    word_vocab_size=WORD_VOCAB_SIZE,\n",
        "    tag_set_size=TAG_SET_SIZE,\n",
        "    label_set_size=LABEL_SET_SIZE,\n",
        "    num_word_features=NUM_WORD_FEATURES,\n",
        "    num_tag_features=NUM_TAG_FEATURES,\n",
        "    num_label_features=NUM_LABEL_FEATURES,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_transitions=NUM_TRANSITIONS\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "# CrossEntropyLoss expects raw logits from the model and target class indices.\n",
        "# It internally applies log-softmax and NLLLoss.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# --- Simulate a batch of data ---\n",
        "# In a real scenario, these would be extracted from parser states.\n",
        "# We use random integers within the valid range for IDs.\n",
        "\n",
        "# Input feature indices (Batch Size, Num Features)\n",
        "dummy_word_indices = torch.randint(0, WORD_VOCAB_SIZE, (BATCH_SIZE, NUM_WORD_FEATURES), dtype=torch.long)\n",
        "dummy_tag_indices = torch.randint(0, TAG_SET_SIZE, (BATCH_SIZE, NUM_TAG_FEATURES), dtype=torch.long)\n",
        "dummy_label_indices = torch.randint(0, LABEL_SET_SIZE, (BATCH_SIZE, NUM_LABEL_FEATURES), dtype=torch.long)\n",
        "\n",
        "# Target transitions (Batch Size) - indices from 0 to NUM_TRANSITIONS-1\n",
        "dummy_target_transitions = torch.randint(0, NUM_TRANSITIONS, (BATCH_SIZE,), dtype=torch.long)\n",
        "\n",
        "# --- Perform a single training step ---\n",
        "\n",
        "# 1. Zero gradients\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# 2. Forward pass: Get logits from the model\n",
        "logits = model(dummy_word_indices, dummy_tag_indices, dummy_label_indices)\n",
        "\n",
        "# 3. Calculate loss\n",
        "loss = criterion(logits, dummy_target_transitions)\n",
        "\n",
        "# 4. Backward pass: Calculate gradients\n",
        "loss.backward()\n",
        "\n",
        "# 5. Update weights\n",
        "optimizer.step()\n",
        "\n",
        "# Print the loss for this step\n",
        "print(f'Simulated training step finished.')\n",
        "print(f'Loss: {loss.item():.4f}')\n",
        "\n",
        "# Optionally, look at the output shape and some logits\n",
        "print(f'\\nLogits shape: {logits.shape}') # Should be (BATCH_SIZE, NUM_TRANSITIONS)\n",
        "# print(f'Example logits: {logits[0]}')\n",
        "\n",
        "# To get probabilities (e.g., for prediction during parsing):\n",
        "probabilities = torch.softmax(logits, dim=1)\n",
        "print(f'\\nProbabilities shape: {probabilities.shape}')\n",
        "# print(f'Example probabilities: {probabilities[0]}')\n",
        "# The predicted transition would be the one with the highest probability:\n",
        "predicted_transitions = torch.argmax(probabilities, dim=1)\n",
        "print(f'Example predicted transition index for first item in batch: {predicted_transitions[0].item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary and Next Steps\n",
        "\n",
        "This notebook successfully implements the feedforward neural network component of a transition-based dependency parser as described in the CS224n notes.\n",
        "\n",
        "**Key Takeaways:**\n",
        "*   Feature templates define which words, tags, and labels from the parser state are used as input.\n",
        "*   Embedding layers convert sparse ID representations into dense vectors.\n",
        "*   Concatenated embeddings form the input to a standard feedforward network.\n",
        "*   The network uses non-linear activations (like the specified cubic function or common ones like ReLU/Tanh) in the hidden layer.\n",
        "*   The output layer produces scores (logits) for each possible parser transition.\n",
        "*   Cross-Entropy loss is suitable for training this multi-class classification problem.\n",
        "\n",
        "**To build a complete parser, one would need to:**\n",
        "1.  Implement the actual arc-standard (or other) transition system (managing stack $\\sigma$, buffer $\\beta$, and arc set $A$).\n",
        "2.  Define a precise feature extraction function that maps a parser state `c = (σ, β, A)` to the required `word_features`, `tag_features`, and `label_features` tensors.\n",
        "3.  Create or obtain a training dataset of sentences annotated with gold dependency trees.\n",
        "4.  Generate gold transition sequences from the gold trees (an \"oracle\").\n",
        "5.  Train the neural network model using the extracted features and corresponding gold transitions from the oracle.\n",
        "6.  Implement the parsing algorithm: Start from an initial state, repeatedly use the trained model to predict the highest-scoring valid transition, apply the transition to update the state, and continue until a terminal state is reached."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
